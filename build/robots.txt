# robots.txt for React Application

# Allow all web crawlers
User-agent: *
Allow: /

# Disallow crawling of certain paths
Disallow: /private/
Disallow: /admin/
Disallow: /api/
Disallow: /internal/
Disallow: /temp/
Disallow: /*.json$
Disallow: /*.xml$

# Sitemap location
Sitemap: https://yourdomain.com/sitemap.xml

# Crawl-delay directive (optional)
Crawl-delay: 10

# Specific rules for major bots
User-agent: Googlebot
Allow: /
Disallow: /nogooglebot/

User-agent: Bingbot
Allow: /
Disallow: /nobingbot/

User-agent: DuckDuckBot
Allow: /
Disallow: /noduckduckbot/

# Block bad bots
User-agent: BadBot
Disallow: /

# Additional common settings
Disallow: /*?*
Disallow: /*?
Disallow: /*&
Disallow: /*.pdf$
Disallow: /*.doc$
Disallow: /*.docx$
Disallow: /*.xls$
Disallow: /*.xlsx$
